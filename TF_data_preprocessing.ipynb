{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning():\n",
    "    df = pd.read_csv(\"crop_yield.csv\", header=0)\n",
    "    df.drop(columns=[\"Crop_Year\", \"Production\"], inplace=True)\n",
    "    df = df[(df[\"Yield\"] > 0.1) & (df[\"Yield\"] <= 150)]\n",
    "    df = df[df[\"Crop\"] != \"Coconut\"]\n",
    "    df[\"Season\"] = df[\"Season\"].str.replace(\" \", \"\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = cleaning()\n",
    "df.to_csv(\"cleaned_crop.csv\", index_label='ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_csv(\"cleaned_crop.csv\", index_col=\"ID\")\n",
    "\n",
    "# # First, let's split the original dataframe into two dataframes, one containing 80% of the data and the other containing 20% of the data.\n",
    "# df_80, df_20 = train_test_split(df, test_size=0.2, stratify=df[\"Crop\"], random_state=0)\n",
    "\n",
    "# # Now let's split df_80 into two dataframes, each containing 40% of the original data.\n",
    "# df_40_1, df_40_2 = train_test_split(\n",
    "#     df_80, test_size=0.5, stratify=df_80[\"Crop\"], random_state=0\n",
    "# )\n",
    "\n",
    "# # Split df_40_1 into two dataframes, each containing 20% of the original data.\n",
    "# train_1, valid_1 = train_test_split(\n",
    "#     df_40_1, test_size=0.5, stratify=df_40_1[\"Crop\"], random_state=0\n",
    "# )\n",
    "\n",
    "# # Split df_40_2 into two dataframes, each containing 20% of the original data.\n",
    "# train_2, train_3 = train_test_split(\n",
    "#     df_40_2, test_size=0.5, stratify=df_40_2[\"Crop\"], random_state=0\n",
    "# )\n",
    "\n",
    "# # Now df_20, train_1, valid_1, train_2, and train_3 are your five dataframes,\n",
    "# # and you can save them to CSV files as follows:\n",
    "# test_1 = df_20\n",
    "# train_1.to_csv(\"train_1.csv\")\n",
    "# train_2.to_csv(\"train_2.csv\")\n",
    "# train_3.to_csv(\"train_3.csv\")\n",
    "# valid_1.to_csv(\"valid_1.csv\")\n",
    "# test_1.to_csv(\"test_1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_TakeDataset element_spec=TensorSpec(shape=(None,), dtype=tf.string, name=None)>\n"
     ]
    }
   ],
   "source": [
    "# file_path = [\"train_1.csv\", \"train_2.csv\", \"train_3.csv\"]\n",
    "\n",
    "\n",
    "# def csv_reader_dataset(\n",
    "#     filepaths,\n",
    "#     n_readers=5,\n",
    "#     n_read_threads=tf.data.AUTOTUNE,\n",
    "#     shuffle_buffer_size=10_000,\n",
    "#     seed=42,\n",
    "#     batch_size=32,\n",
    "# ):\n",
    "#     dataset = tf.data.Dataset.list_files(filepaths)\n",
    "\n",
    "#     dataset = dataset.interleave(\n",
    "#         lambda filepath: tf.data.TextLineDataset(filepath).skip(1),\n",
    "#         cycle_length=n_readers, num_parallel_calls=n_read_threads)\n",
    "\n",
    "#     dataset = dataset.shuffle(shuffle_buffer_size, seed = seed)\n",
    "\n",
    "#     return dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "\n",
    "# test_df = csv_reader_dataset(file_path)\n",
    "\n",
    "# print(test_df.take(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "X = df.drop(columns=\"Yield\")\n",
    "y = df[\"Yield\"].copy()\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the original data into training (70%) and temporary test (30%) sets\n",
    "X_train, X_temp, Y_train, Y_temp = train_test_split(\n",
    "    X, y, test_size=0.3, stratify=df[\"Crop\"], random_state=0\n",
    ")\n",
    "\n",
    "# Now split the temporary test set into validation (15%) and test (15%) sets\n",
    "X_valid, X_test, Y_valid, Y_test = train_test_split(\n",
    "    X_temp, Y_temp, test_size=0.5, stratify=X_temp[\"Crop\"], random_state=0\n",
    ")\n",
    "\n",
    "\n",
    "def apply_log1p_to_numeric(df):\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    df[numeric_cols] = df[numeric_cols].apply(np.log1p)\n",
    "    return df\n",
    "\n",
    "\n",
    "X_train_logged = apply_log1p_to_numeric(X_train.copy())\n",
    "X_valid_logged = apply_log1p_to_numeric(X_valid.copy())\n",
    "X_test_logged = apply_log1p_to_numeric(X_test.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "422/422 [==============================] - 8s 15ms/step - loss: 83.8844 - root_mean_squared_error: 9.1588 - val_loss: 55.2190 - val_root_mean_squared_error: 7.4309 - lr: 0.0010\n",
      "Epoch 2/20\n",
      "422/422 [==============================] - 3s 8ms/step - loss: 54.3094 - root_mean_squared_error: 7.3695 - val_loss: 49.5871 - val_root_mean_squared_error: 7.0418 - lr: 0.0019\n",
      "Epoch 3/20\n",
      "422/422 [==============================] - 3s 8ms/step - loss: 67.8975 - root_mean_squared_error: 8.2400 - val_loss: 54.9844 - val_root_mean_squared_error: 7.4151 - lr: 0.0028\n",
      "Epoch 4/20\n",
      "422/422 [==============================] - 3s 8ms/step - loss: 90.7543 - root_mean_squared_error: 9.5265 - val_loss: 107.8284 - val_root_mean_squared_error: 10.3840 - lr: 0.0037\n",
      "Epoch 5/20\n",
      "422/422 [==============================] - 3s 8ms/step - loss: 172.7436 - root_mean_squared_error: 13.1432 - val_loss: 86.0633 - val_root_mean_squared_error: 9.2770 - lr: 0.0046\n",
      "Epoch 6/20\n",
      "422/422 [==============================] - 3s 8ms/step - loss: 327.1714 - root_mean_squared_error: 18.0879 - val_loss: 152.4641 - val_root_mean_squared_error: 12.3476 - lr: 0.0055\n",
      "Epoch 7/20\n",
      "422/422 [==============================] - 3s 8ms/step - loss: 326.6858 - root_mean_squared_error: 18.0745 - val_loss: 258.3309 - val_root_mean_squared_error: 16.0727 - lr: 0.0064\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_hub as hub\n",
    "\n",
    "\n",
    "norm_layer = tf.keras.layers.Normalization()\n",
    "norm_layer.adapt(\n",
    "    X_train_logged.select_dtypes(include=[np.number])\n",
    ")  # Correctly adapt on the logged training data\n",
    "\n",
    "# Create one-hot encoding layers and adapt them\n",
    "one_hot_layer_1 = tf.keras.layers.StringLookup(output_mode=\"one_hot\")\n",
    "one_hot_layer_1.adapt(df[\"State\"])\n",
    "one_hot_layer_2 = tf.keras.layers.StringLookup(output_mode=\"one_hot\")\n",
    "one_hot_layer_2.adapt(df[\"Season\"])\n",
    "\n",
    "embeddings_layer = hub.KerasLayer(\"https://tfhub.dev/google/nnlm-en-dim50/2\")\n",
    "\n",
    "input_numeric = tf.keras.Input(\n",
    "    shape=(X_train.shape[1] - 3,), dtype=tf.float32, name=\"NumericData\"\n",
    ")\n",
    "input_text1 = tf.keras.Input(shape=(1,), dtype=tf.string, name=\"State\")\n",
    "input_text2 = tf.keras.Input(shape=(1,), dtype=tf.string, name=\"Season\")\n",
    "input_text3 = tf.keras.Input(shape=(1,), dtype=tf.string, name=\"Crop\")\n",
    "\n",
    "\n",
    "norm_numeric = norm_layer(input_numeric)\n",
    "ohe_hot_state = one_hot_layer_1(input_text1)\n",
    "one_hot_season = one_hot_layer_2(input_text2)\n",
    "\n",
    "################\n",
    "input_text3 = tf.keras.Input(shape=(1,), dtype=tf.string, name=\"Crop\")\n",
    "input_text3_squeezed = tf.squeeze(input_text3, axis=1)\n",
    "embedded_crop = embeddings_layer(input_text3_squeezed)\n",
    "# embedded_crop = embeddings_layer(input_text3)\n",
    "################\n",
    "\n",
    "\n",
    "\n",
    "concat_input = tf.keras.layers.concatenate(\n",
    "    [norm_numeric, ohe_hot_state, one_hot_season, embedded_crop]\n",
    ")\n",
    "\n",
    "dense_1 = tf.keras.layers.Dense(128, activation=\"relu\", kernel_initializer=\"he_normal\")(\n",
    "    concat_input\n",
    ")\n",
    "dense_2 = tf.keras.layers.Dense(64, activation=\"relu\", kernel_initializer=\"he_normal\")(\n",
    "    dense_1\n",
    ")\n",
    "output_layer = tf.keras.layers.Dense(1)(dense_2)\n",
    "\n",
    "model = tf.keras.Model(\n",
    "    inputs=[input_numeric, input_text1, input_text2, input_text3], outputs=output_layer\n",
    ")\n",
    "\n",
    "# Comile the model\n",
    "# optimizer=tf.keras.optimizers.legacy.SGD(momentum=0.9, nesterov=True)\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss=\"mse\",\n",
    "    metrics=[\"RootMeanSquaredError\"],\n",
    ")\n",
    "\n",
    "\n",
    "# Fit the model\n",
    "\n",
    "input_data = {\n",
    "    \"NumericData\": X_train_logged.select_dtypes(include=[np.number]),\n",
    "    \"State\": X_train_logged[\"State\"],\n",
    "    \"Season\": X_train_logged[\"Season\"],\n",
    "    \"Crop\": X_train_logged[\"Crop\"],\n",
    "}\n",
    "\n",
    "\n",
    "validation_data = (\n",
    "    {\n",
    "        \"NumericData\": X_valid_logged.select_dtypes(include=[np.number]),\n",
    "        \"State\": X_valid_logged[\"State\"],\n",
    "        \"Season\": X_valid_logged[\"Season\"],\n",
    "        \"Crop\": X_valid_logged [\"Crop\"],\n",
    "    },\n",
    "    Y_valid,\n",
    ")\n",
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True)\n",
    "\n",
    "\n",
    "def one_cycle_scheduler(epoch, lr):\n",
    "    max_lr = 0.01  # Set your maximum learning rate\n",
    "    base_lr = 0.001  # Set your base (minimum) learning rate\n",
    "    total_epochs = 20  # Set your total number of training epochs\n",
    "\n",
    "    if epoch < total_epochs / 2:\n",
    "        # Linearly increase the learning rate from base_lr to max_lr\n",
    "        return base_lr + (max_lr - base_lr) * (epoch / (total_epochs / 2))\n",
    "    else:\n",
    "        # Linearly decrease the learning rate from max_lr to base_lr\n",
    "        return max_lr - (max_lr - base_lr) * (\n",
    "            (epoch - total_epochs / 2) / (total_epochs / 2)\n",
    "        )\n",
    "\n",
    "# Create the LearningRateScheduler callback\n",
    "lr_schedule_callback = tf.keras.callbacks.LearningRateScheduler(one_cycle_scheduler)\n",
    "\n",
    "history = model.fit(\n",
    "    input_data,\n",
    "    Y_train,\n",
    "    epochs=20,\n",
    "    validation_data=validation_data,\n",
    "    callbacks=[early_stopping, lr_schedule_callback],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(13497, 50), dtype=float32, numpy=\n",
       "array([[ 0.2216289 , -0.33971202,  0.199434  , ...,  0.04292737,\n",
       "        -0.2232489 , -0.19333361],\n",
       "       [ 0.07214864, -0.04171149, -0.00954658, ...,  0.15353486,\n",
       "        -0.00463091,  0.10250401],\n",
       "       [-0.0877428 , -0.18014443,  0.07023089, ...,  0.14613886,\n",
       "        -0.06293976,  0.09830101],\n",
       "       ...,\n",
       "       [ 0.2216289 , -0.33971202,  0.199434  , ...,  0.04292737,\n",
       "        -0.2232489 , -0.19333361],\n",
       "       [-0.02339478, -0.19061647,  0.03753536, ..., -0.02141087,\n",
       "         0.06894715, -0.11102317],\n",
       "       [-0.00661174, -0.21552294, -0.21807763, ...,  0.18313994,\n",
       "        -0.09194405, -0.06275211]], dtype=float32)>"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_layer(X_train_logged[\"Crop\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
